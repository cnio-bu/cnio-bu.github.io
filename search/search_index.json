{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CNIO Bioinformatics Unit documentation","text":"<p>Welcome to the documentation repository of the CNIO Bioinformatics Unit.</p> <ul> <li> <p>To navigate the documentation just use the menu on the left side of the page.</p> </li> <li> <p>You can check the source code that generates this site here.</p> </li> </ul>"},{"location":"dev/","title":"Guidelines for collaboration when writing code","text":"<p>Here are some guidelines for writing and managing code. You should be familiar with all the concepts in all the text and links in this page (except for the examples for code structure, which are on a need-to-know basis, and the source pages from the extracted paragraphs that you will find pasted inline).</p>"},{"location":"dev/#using-version-control-git","title":"Using version control (git)","text":"<p>We use git for version control. If you're not familiar with git, this is a good place to start, and this is a fantastic follow-up. </p> <p>There are some good interactive resources to learn git in general, and git branching in particular.</p>"},{"location":"dev/#where-to-put-your-code","title":"Where to put your code","text":"<p>We have a github group for the Unit for hosting and managing our code.</p> <p>If you're a member or collaborator of the Bioinformatics Unit and wish to gain access, you simply need to create a GitHub user and either contact a BU member, or create an issue to request access.</p>"},{"location":"dev/#how-to-organise-and-manage-your-code","title":"How to organise and manage your code","text":"<p>In order to facilitate collaboration when more than one person are working on a project, we follow some rules for repository structure and git-related operations (branches, commits, merge requests, etc.).</p>"},{"location":"dev/#project-structure","title":"Project structure","text":"<p>The structure of the repository will be defined by the project owner, ideally following well-established guidelines (see examples for Snakemake, Python, and R.</p>"},{"location":"dev/#git-workflow","title":"git workflow","text":"<p>For git, we follow the GitHub flow model. It's a simple and concise model that (at the time of writing) perfectly suits our requirements.</p>"},{"location":"dev/#commits","title":"Commits","text":"<p>Some more details on how we do commits and merge them into master are available here.</p>"},{"location":"dev/#merge-requests","title":"Merge requests","text":"<p>Start here to understand why merge requests, similarly to commits, need to be \"atomic\".</p> <p>Here are two paragraphs extracted from other sites with some additional details on merge requests:</p> <p>Adapted from https://alexsav.io/git-collaboration-guidelines.html</p> <p>The Pull or Merge Requests (PR) are used to share with your co-developers the code changes you did in your branch and ask them to review it. Here we always do a PR from your branch to the \u2018master\u2019 branch. In a PR, your colleagues will have a view of the differences between the branches and the commits. You must also add a title and a description. The title should be sufficient to understand what is being changed. In the description you should: - make a useful description, - describe what was changed in the pull request, - explain why this PR exists, - make it clear how it does what it sets out to do. E.g: Does it change a column in the database? How is this being done? What happens to the old data? - you may want to use screenshots to demonstrate what has changed if there is a GUI involved in the project.</p> <p>Single Responsibility Principle: The pull request should do only 1 thing.</p> <p>Pull request size: It should be small. The pull request must have a maximum of approximately 250 lines of change.</p> <p>Feature breaking: Whenever it\u2019s possible break pull requests into smaller ones.</p> <p>Adapted from https://yalantis.com/blog/code-review-via-gitlab-merge-requests-code-review-must/</p> <p>To minimize the time spent on reviewing each merge request, you need to have a strategy for code review.</p> <p>In our humble opinion, a good developer is not just someone who follows a programming workflow and writes high-quality code. A good developer knows how to deliver code for review and make the whole code review process effortless for the reviewer.</p> <p>Keep in mind that a good merge request should solve a specific task. We suggest not including more than one feature in one merge request. It would be much better if you created several merge requests instead.</p> <p>As a reviewer, don\u2019t hesitate to pull the source branch and test incoming code by yourself, especially if the merge request contains plenty of changes. Build the project and check that everything works as expected.</p> <p>Also, an important detail in our code review checklist is deleting branches when they\u2019re no longer needed. The responsibility for deleting branches after they\u2019re fully merged lies with the reviewer.</p>"},{"location":"admin/inventory/","title":"Workstations","text":"User Name Area Machine IP chernandez Carolina Hern\u00e1ndez Oliver Grupo de Met\u00e1stasis Cerebral / Unidad de Bioinform\u00e1tica caurel 10.222.112.28 mtress Michael Tress Unidad de Bioinform\u00e1tica andes 10.222.112.32 fsoriano Francisco Javier Soriano D\u00edaz Unidad de Bioinform\u00e1tica tambora 10.222.112.33 lmartinezg Laura Mart\u00ednez g\u00f3mez Unidad de Bioinform\u00e1tica urales 10.222.112.35 dcerdan Daniel Cerd\u00e1n V\u00e9lez Unidad de Bioinform\u00e1tica jura 10.222.112.37 epineiro Elena Pi\u00f1eiro Ya\u00f1ez Unidad de Bioinform\u00e1tica meira 10.222.112.39 dcerdan Daniel Cerd\u00e1n V\u00e9lez Unidad de Bioinform\u00e1tica gazpacho 10.222.112.43 dcerdan Daniel Cerd\u00e1n V\u00e9lez Unidad de Bioinform\u00e1tica maliciosa 10.222.112.53 mjjimenez Mar\u00eda Jos\u00e9 J\u00edmenez Santos Unidad de Bioinform\u00e1tica kilimanjaro 10.222.112.60 lolle Laia Oll\u00e9 Monr\u00e0s Grupo de Computacional Cancer Genomics anakin 10.222.112.68 mjjimenez Mar\u00eda Jos\u00e9 J\u00edmenez Santos Unidad de Bioinform\u00e1tica windu 10.222.112.85 gpiedrafita Gabriel Piedrafita Fern\u00e1ndez Grupo de Carcinog\u00e9nesis Epitelial / Unidad de Bioinform\u00e1tica patterson 10.222.112.97 ggomez Gonzalo G\u00f3mez L\u00f3pez Unidad de Bioinform\u00e1tica quigon 10.222.112.100 jmartinezv Jaime Mart\u00ednez de Villarreal Chico Grupo de Carcinog\u00e9nesis Epitelial gurugu 10.222.112.135 fpozoc Fernando Pozo Ocampo Unidad de Bioinform\u00e1tica jonflash 10.222.112.143 ccarreterop Carlos Carretero Puche Unidad de Investigaci\u00f3n Cl\u00ednica de C\u00e1ncer Pulm\u00f3n H12O-CNIO monete 10.222.112.149 ralvarezd Ruth Alvarez D\u00edaz Grupo de Oncolog\u00eda Experimental terril 10.222.113.25 tdidomenico Tom\u00e1s Di Domenico Unidad de Bioinform\u00e1tica viper 10.222.113.30 sagarcia Santiago Garc\u00eda Mart\u00edn Unidad de Bioinform\u00e1tica azura 10.222.113.32 mmoradiellos Manuel Moradiellos Corpus Grupo de Computacional Cancer Genomics gojira 10.222.113.27 lserranor Laura Serrano Ron Unidad de Bioinform\u00e1tica rafiki 10.222.113.129 - - - himalaya 10.222.112.49 - - - alytes 10.222.112.109 - - - moriarti 10.222.112.142 - - - tomate 10.222.112.139 - - - enllamas -"},{"location":"admin/tasks/","title":"Setting up workstation users","text":"<p>Note</p> <p>If it's a new user, you'll need to send a ticket (incidencia) to IT for them to a) enable the user for Linux and b) create the shared home (specify both).</p> <p>Warning</p> <p>The cnio username (\"xxxx\" in \"xxxx@cnio.es\") should not exist locally on the machine or it will clash with the remote one. If you used the same username for the installation of the OS you will first need to remove it from the local machine.</p>"},{"location":"admin/tasks/#ldap","title":"LDAP","text":""},{"location":"admin/tasks/#install-required-packages-leave-all-options-as-default-when-prompted","title":"Install required packages (leave all options as default when prompted)","text":"<pre><code>$ sudo apt-get update\n$ sudo apt-get -y install libnss-ldap libpam-ldap ldap-utils nscd\n</code></pre>"},{"location":"admin/tasks/#update-etcnsswitchconf","title":"Update /etc/nsswitch.conf","text":"<pre><code>passwd:         compat ldap\ngroup:          compat ldap\nshadow:         compat ldap\n</code></pre>"},{"location":"admin/tasks/#replace-contents-in-etcldapconf","title":"Replace contents in /etc/ldap.conf","text":"<pre><code>HOST cnio.es\n#***** HOST seth.cnio.es\n\n#BASE DC=cnio,DC=es\nBASE OU=Usuarios,OU=Programa Biologia Estructural,OU=CNIO.ES,DC=cnio,DC=es\n\nbinddn cn=bioldap,cn=Users,dc=cnio,dc=es\nbindpw #####REDACTED######\n\nnss_base_passwd OU=Usuarios,OU=Programa Biologia Estructural,OU=CNIO.ES,DC=cnio,DC=es?sub\nnss_base_passwd OU=Usuarios,OU=Programa Biotecnologia,OU=CNIO.ES,DC=cnio,DC=es\nnss_base_passwd OU=Usuarios,OU=Programa Terapias Experimentales,OU=CNIO.ES,DC=cnio,DC=es\nnss_base_passwd DC=cnio,DC=es?sub\nnss_base_shadow OU=Usuarios,OU=Programa Biologia Estructural,OU=CNIO.ES,DC=cnio,DC=es?sub\nnss_base_shadow OU=Usuarios,OU=Programa Biotecnologia,OU=CNIO.ES,DC=cnio,DC=es\nnss_base_shadow OU=Usuarios,OU=Programa Terapias Experimentales,OU=CNIO.ES,DC=cnio,DC=es\nnss_base_shadow DC=cnio,DC=es?sub\nnss_base_group OU=PBE,OU=ACLs,DC=cnio,DC=es?sub\nnss_map_objectclass posixAccount User\nnss_map_objectclass shadowAccount User\nnss_map_attribute uid sAMAccountName\nnss_map_attribute uidNumber msSFU30UidNumber\nnss_map_attribute gidNumber msSFU30GidNumber\nnss_map_attribute homeDirectory msSFU30HomeDirectory\nnss_map_attribute loginShell msSFU30LoginShell\nnss_map_objectclass posixGroup Group\nnss_map_attribute cn sAMAccountName\npam_login_attribute sAMAccountName\npam_filter objectclass=user\npam_password ad  \nnss_initgroups_ignoreusers avahi,avahi-autoipd,backup,bin,colord,daemon,debian-spamd,dhcpd,dnsmasq,elasticsearch,games,gdm,gnats,guest-ngHhtq,hplip,irc,kernoops,libuuid,lightdm,list,lp,mail,man,messagebus,mysql,nagios,news,postfix,postgres,proxy,pulse,root,rstudio-server,rtkit,saned,sgeadmin,speech-dispatcher,sshd,statd,sync,sys,syslog,tftp,usbmux,uucp,whoopsie,www-data\n</code></pre>"},{"location":"admin/tasks/#restart-nscd-service","title":"Restart nscd service","text":"<pre><code>$ sudo service nscd restart\n</code></pre>"},{"location":"admin/tasks/#verify-ldap-login","title":"Verify LDAP login","text":"<pre><code>$ getent passwd ldapuser\n\nldapuser:x:9999:100:Test LdapUser:/home/ldapuser:/bin/bash\n</code></pre>"},{"location":"admin/tasks/#cnio-shared-homes","title":"CNIO shared homes","text":"<p>Home mounting was originally done with autofs. It does not play well with systemd and the long delays on network uplinks, so it's a bit of a mess in Ubuntu (tested on 18). The \"classic\" fstab mounting is therefore preferred.</p> <p>Warning</p> <p>The \"classic\" and the autofs mounts are mutually exclusive. You need to set up one or the other. Otherwise you'll have conflicts and problems will arise.</p>"},{"location":"admin/tasks/#classic-mount","title":"Classic mount","text":""},{"location":"admin/tasks/#install-nfs","title":"Install nfs","text":"<p>Install nfs-common if not already available:</p> <pre><code>apt-get install nfs-common\n</code></pre>"},{"location":"admin/tasks/#edit-etcfstab","title":"Edit /etc/fstab","text":"<pre><code>lando.cnio.es:/homes/&lt;user&gt; /home/&lt;user&gt; nfs  auto,noatime,nolock,bg,nfsvers=3,tcp,intr,_netdev,x-systemd.automount,x-systemd.after=network-online.target,x-systemd.device-timeout=240      0       0\n</code></pre>"},{"location":"admin/tasks/#autofs","title":"autofs","text":""},{"location":"admin/tasks/#install-required-packages","title":"Install required packages","text":"<pre><code>$ sudo apt-get -y install autofs\n</code></pre>"},{"location":"admin/tasks/#add-a-home-alias-to-etcautomaster-if-its-not-there-yet","title":"Add a /home alias to /etc/auto.master (if it's not there yet)","text":"<pre><code>$ vi /etc/auto.master\n...\n/home /etc/auto.home\n...\n</code></pre>"},{"location":"admin/tasks/#add-the-user-to-the-home-automount-list","title":"Add the user to the home automount list","text":"<pre><code>$ vi /etc/auto.home\n...\nusername      lando.cnio.es:/homes/username\n...\n</code></pre>"},{"location":"hpc/admin/","title":"Cluster administration","text":""},{"location":"hpc/admin/#rack-layout","title":"Rack layout","text":"<p>Diagram can be edited with diagrams.net using the source code available here</p>"},{"location":"hpc/admin/#checking-and-restarting-nodes","title":"Checking and restarting nodes","text":"<p>For a full list of all available nodes, type:</p> <pre><code>scontrol show node\n</code></pre> <p>A given node status can be checked out with:</p> <pre><code>scontrol show node NODENAME\n</code></pre> <p>Sometimes, a node state will change to DRAIN for multiple reasons: i.e. slurm could not gracefully kill a job on a given node. Drained nodes won't handle incoming jobs until they are manually reset. To do so, the following command must be typed with admin privileges: </p> <pre><code>scontrol update NodeName=NODENAME State=RESUME\n</code></pre>"},{"location":"hpc/usage/","title":"The CNIO HPC cluster","text":"<p>These are some guidelines to using our HPC cluster and the Slurm workflow system.</p>"},{"location":"hpc/usage/#cluster-resources","title":"Cluster resources","text":""},{"location":"hpc/usage/#compute-nodes","title":"Compute nodes","text":"<p>The cluster currently features 12 compute nodes with the following configurations:</p> Count Node names CPU cores RAM GPUs 1 bc001 24 32Gb    - 6 bc00[2-7] 52 512Gb - 3 bc00[8-10] 128 1Tb - 1 hm001 224 2Tb - 1 gp001 112 768Gb 3 x Nvidia A100 80Gb <p></p>"},{"location":"hpc/usage/#storage","title":"Storage","text":"<p>The cluster features 54Tb of standard storage (where user's home directories are mounted) and 512Tb of high performance storage (for compute jobs input and output).</p>"},{"location":"hpc/usage/#paragraph-for-adding-to-grant-applications-etc","title":"Paragraph for adding to grant applications, etc.","text":"<p>The CNIO HPC cluster currently consists of: two login nodes working in active/passive mode (2x 40 core/392Gb RAM); a total of 728 compute cores distributed in 9 \"standard\" compute nodes (6x 52 core/512Gb RAM and 3x 64core/768Gb RAM) and a high-memory node (1x 224 core/2Tb RAM); a dedicated GPU node featuring 3 x Nvidia A100 80Gb GPUs. Local storage is composed of 48 4TB disks in a dual-channel enclosure organized into a RAID-10 unit with an effective available storage of 54TB which host the home directories, and a high-performance lustre storage system with 512 TB of effective available storage for computation. The cluster runs a Slurm queuing system with fairshare priority management.</p>"},{"location":"hpc/usage/#cluster-usage","title":"Cluster usage","text":""},{"location":"hpc/usage/#introduction-to-hpc","title":"Introduction to HPC","text":"<p>See this tutorial for an introduction to HPC concepts and usage (highly recommended if you have little or no experience in using HPC environments).</p>"},{"location":"hpc/usage/#requesting-access","title":"Requesting access","text":"<p>Please fill out the access request form in order to  request access to the cluster.</p>"},{"location":"hpc/usage/#accessing-the-cluster","title":"Accessing the cluster","text":"<p>The cluster's hostname is <code>cluster1.cnio.es</code> (<code>cluster1</code> for short), and can be accessed via SSH.</p> <p>Note</p> <p>If you'd like to SSH into the cluster through the CNIO VPN you will need to ask IT for VPN access.</p>"},{"location":"hpc/usage/#mailing-list","title":"Mailing list","text":"<p>Important notifications about the cluster are sent to the CNIO HPC mailing list. Make sure to subscribe using this link to stay up to date. The list has very low traffic.</p> <p>Note</p> <p>Do subscribe to the list!</p>"},{"location":"hpc/usage/#storage_1","title":"Storage","text":"<p>Warning</p> <p>There are two types of storage in the cluster: your home directory and scratch space.</p> <p>Your home directory is well suited for storing software (e.g. installing conda), configuration files, etc. It is however not quick enough to store the input or output files of your analyses. Using your home for this will likely hang your jobs and corrupt your files. You should use the scratch space instead.</p> <p>Warning</p> <p>Scratch space is to be considered \"volatile\": you should copy your input files, execute, copy your output files, and delete everything.</p> <p>Home directories are not volatile, but data safety is not guaranteed in the long term (so keep backups).</p>"},{"location":"hpc/usage/#home-directories","title":"Home directories","text":"<p>You have an allocation of 300Gb in your home directory, located in <code>/home/&lt;yourusername&gt;/</code>.</p>"},{"location":"hpc/usage/#scratch-space","title":"Scratch space","text":""},{"location":"hpc/usage/#user-space","title":"User space","text":"<p>You have an allocation of 500Gb in your \"scratch\" directory, located at <code>/storage/scratch01/users/&lt;yourusername&gt;/</code>.</p> <p>This space is meant to be used for testing and ephemeral compute operations.</p>"},{"location":"hpc/usage/#project-space","title":"Project space","text":"<p>To request high-performance scratch space for a new project please fill the storage request form.</p>"},{"location":"hpc/usage/#checking-your-quotas","title":"Checking your quotas","text":"<p>You can check your current quotas with the following commands:</p> <pre><code>$ zfs get -r -o name,property,value userquota@$(whoami),userused@$(whoami) homepool2/home\n\n$ quotr quota list --long # check your scratch quotas\n\n</code></pre>"},{"location":"hpc/usage/#copying-data-tofrom-the-cluster","title":"Copying data to/from the cluster","text":"<p>The most efficient way of copying large amounts of data is by using <code>rsync</code>. <code>rsync</code> allows you to resume a transfer in case something goes wrong.</p> <p>To transfer a directory to the cluster using <code>rsync</code> you would do something like:</p> <pre><code>rsync -avx --progress mydirectory/ cluster1:/storage/scratch01/myuser/mydirectory/\n</code></pre> <p>Note</p> <p>Pay attention to the trailing slashes, which completely change rsync's behaviour if missing.</p> <p>For small transfers you could also use <code>scp</code> if you prefer:</p> <pre><code>scp -r mydirectory/ cluster1:/storage/scratch01/myuser/\n</code></pre> <p>In both cases you can revert the order of the local and remote directory to copy from the cluster to your local computer instead.</p>"},{"location":"hpc/usage/#submitting-jobs","title":"Submitting jobs","text":"<p>Warning</p> <p>As a general rule, no heavy processes should be run directly on the login nodes. Instead you should use the \"sbatch\" or \"srun\" commands to send them to the compute nodes. See examples below.</p> <p>The command structure to send a job to the queue is the following:</p> <pre><code>sbatch -o &lt;logfile&gt; -e &lt;errfile&gt; -J &lt;jobname&gt; -c &lt;ncores&gt; --mem=&lt;total_memory&gt;G \\\n    -t&lt;time_minutes&gt; --wrap \"&lt;command&gt;\"\n</code></pre> <p>For example, to submit the command <code>bwa index mygenome.fasta</code> as a job: </p> <pre><code>sbatch -o log.txt -e error.txt -J index_genome -c 1 --mem=4G -t120 \\\n    --wrap \"bwa index mygenome.fasta\"\n</code></pre>"},{"location":"hpc/usage/#job-resources","title":"Job resources","text":"<p><code>-c</code>, <code>--mem</code>, and <code>-t</code> tell the system how many cores, RAM memory, and time your job will need.</p> <p>Note</p> <p>Unless otherwise specified, jobs will get a default of 1 core, 2Gb of RAM, and a time limit of 30 minutes.</p>"},{"location":"hpc/usage/#thread-management","title":"Thread management","text":"<p>Many programs can spawn additional threads beyond what you explicitly request through the <code>-c</code> parameter. This can lead to oversubscription of CPU resources and negatively impact both your job and others running on the same node. For example, some Python libraries (like NumPy), R packages, or bioinformatics tools might automatically use multiple threads based on the available CPU cores, regardless of your Slurm allocation.</p> <p>To prevent this:</p> <ul> <li>Look for program-specific thread control options (e.g., <code>--threads</code>, <code>-t</code>, etc.)</li> <li>If applicable, consider setting environment variables like <code>OMP_NUM_THREADS</code>, <code>OPENBLAS_NUM_THREADS</code>, <code>MKL_NUM_THREADS</code>, and <code>NUMEXPR_NUM_THREADS</code> to match your requested cores</li> <li>When using workflow managers like Snakemake or Nextflow, ensure they properly propagate resource constraints to their child processes</li> </ul> <p>Please check this message on the mailing list for more details and an example.</p>"},{"location":"hpc/usage/#time-limits","title":"Time limits","text":"<p>Note</p> <p>Ideally, you should aim at your jobs having a duration of between 1 and 8 hours. Shorter jobs may cause too much scheduling overhead, and longer jobs will make it harder for the scheduler to optimally schedule jobs into the queues.</p> <p>The \"short\" queue has a limit of 2 hours per job and the highest priority (i.e. jobs in this queue will run sooner compared to other queues).</p> <p>The \"main\" queue has a limit of 24 hours per job and medium priority. </p> <p>The \"long\" queue has a limit of 168 hours (7 days) per job and 4 concurrent jobs, and the lowest priority.</p> <p>Note</p> <p>You can specify the partition (queue) you want to use with the <code>-p</code> argument to <code>sbatch</code> or <code>srun</code>.</p>"},{"location":"hpc/usage/#resources-and-their-effect-on-job-priority","title":"Resources and their effect on job priority","text":"<p>The resources you request for a job will influence the chances that such job has to enter the queue, compared to others: the more resources you request, the longer you may have to wait for those resources to be available.</p> <p>In addition, the future priority of your jobs will also be influenced by the resources you request (not use, request, even if you don't use them in the end): the more you request, the less priority you'll have for future jobs.</p> <p>If one of your jobs has lower priority than another one, but its running time would not delay that higher priority one from entering the queue and there are enough resources for it, your job could enter the queue first. That's why it's important to try and assign reasonably accurate time limits (a.k.a. walltimes) to your jobs.</p> <p>You can check how efficiently a job used its assigned resources with the <code>seff &lt;jobid&gt;</code> command (once the job is finished).</p>"},{"location":"hpc/usage/#snakemake-executor-for-slurm","title":"Snakemake executor for Slurm","text":"<p>Warning</p> <p>The previous way of having Snakemake send jobs to the nodes using a profile (<code>--profile $SMK_PROFILE_SLURM</code>) is still functional but deprecated. You should ideally move to the native executor described below, and report any issues to the list. </p> <p>Snakemake features a Slurm executor plugin, which will send jobs to Slurm instead of running them locally. After installing it, simply add the <code>--executor slurm</code> argument to your <code>snakemake</code> command.</p> <p>Note</p> <p>In order to indicate the resources required by each Snakemake rule you should use the threads and resources options.</p> <p>Note</p> <p>The Snakemake command will remain active while your jobs run, so it's recommended that you launch it inside a detachable terminal emulator (e.g. GNU Screen) so you can disconnect from the cluster and keep your jobs running.</p>"},{"location":"hpc/usage/#getting-information-for-jobs-submitted-via-snakemake","title":"Getting information for jobs submitted via Snakemake","text":"<p>Text extracted from the Snakemake Slurm executor docs</p> <p>The executor plugin for SLURM uses unique job names to inquire about job status. It ensures inquiring about job status for the series of jobs of a workflow does not put too much strain on the batch system\u2019s database. Human readable information is stored in the comment of a particular job. It is a combination of the rule name and wildcards. You can ask for it with the sacct or squeue commands, for example:</p> <pre><code>sacct -o JobID,State,Comment%40\n</code></pre> <p>Note, the \u201c%40\u201d after Comment ensures a width of 40 characters. This setting may be changed at will. If the width is too small, SLURM will abbreviate the column with a + sign.</p> <p>For running jobs, you can use the squeue command:</p> <pre><code>squeue -u $USER -o %i,%P,%.10j,%.40k\n</code></pre> <p>Here, the . settings for the ID and the comment ensure a sufficient width, too."},{"location":"hpc/usage/#interactive-sessions","title":"Interactive sessions","text":"<p>During development and testing you may want to be able to run commands interactively. You can request an interactive session, which will run on a compute node, using the <code>srun</code> command with the <code>--pty</code> argument.</p> <p>For example, to request a 2-hour session with 4Gb RAM and 2 CPUs, you would do:</p> <p><code>srun --mem=4096 -c 2 -t 120 --pty /bin/bash</code></p> <p>Note</p> <p>Interactive sessions are limited to a maximum of 120 minutes.</p>"},{"location":"hpc/usage/#gpus","title":"GPUs","text":"<p>Warning</p> <p>GPU usage is \"experimental\". Please expect changes in the setup (which will be announced through the mailing list).</p> <p>The cluster features three Nvidia A100 GPUs with 80Gb of VRAM each.</p> <p>To run a command using GPUs you need to specify the <code>gpu</code> partition, and request the number of GPUs you require using the <code>--gres=gpu:A100:&lt;number_of_gpus&gt;</code> argument of <code>sbatch</code>. Here's an example to run a python script with one GPU:</p> <p><code>sbatch -p gpu --gres=gpu:A100:1 --wrap \"python train_my_net.py\"</code></p>"},{"location":"hpc/usage/#installing-software","title":"Installing software","text":"<p>Software management is left up to the user, and we recommend doing it by installing miniforge and the bioconda channels.</p> <p>If you're completely unfamiliar with conda and bioconda, we recommend following this tutorial.</p> <p>Note</p> <p>Remember to use your home directory to install software (including conda), as lustre performs poorly when reading small files often.</p>"},{"location":"org/calendar/","title":"Outlook Calendar","text":"<p>CNIO provides access to Microsoft 365 and its suite of productivity tools. The Bioinformatics Unit calendar centralises all events that are relevant to the whole Unit.</p> <p>Warning</p> <p>The Bioinformatics Unit group calendar may not be visible for you by default. Make sure you enable it in Outlook by selecting it in your list of calendars.</p>"},{"location":"org/calendar/#creating-events","title":"Creating Events","text":"<p>The idea is for the person responsible for an event to add it to the calendar, inviting those directly involved. The rest of the Unit members will then be aware of what's going on by having the events visible on their calendars.</p> <p>In short:</p> <ul> <li>Events of general interest to the Unit (lab meetings, project meetings, conferences, etc.) should be created in the Bioinformatics Unit group calendar.</li> <li>The person responsible for a meeting should be the one creating the calendar event.</li> <li>People directly involved should be invited to the event.</li> <li>The \"Teams\" meeting option should be enabled to allow for remote participation (unless otherwise agreed).</li> </ul>"},{"location":"org/calendar/#synchronisation-with-3rd-party-clients","title":"Synchronisation with 3rd Party Clients","text":"<p>Group calendars are not exposed by the Exchange protocol, and therefore can't be synced with 3rd party clients.</p> <p>A workaround for this is to click on the event and select \"Add to my calendar\". This should copy the event to your personal calendar and keep it in sync (YMMV, as this is a Microsoft).</p>"},{"location":"org/chat/","title":"Matrix Chat","text":"<p>We use Matrix Chat for internal communications within the Bioinformatics Unit.</p> <p>Matrix is a federated chat protocol that allows secure, decentralized communication. Users can create accounts on one server and join channels from any server on the federation.</p>"},{"location":"org/chat/#creating-an-account","title":"Creating an Account","text":"<p>To create an account, choose one of the available Matrix clients. The web-based version of Element is a good place to start. Choose the public matrix.org server when creating your account. After creating your account, feel free to install the Android or iOS versions, or explore other clients.</p> <p>Warning</p> <p>Make sure to create a backup of your encryption keys and keep the associated recovery key safe. Otherwise, you won't be able to access old messages after logging in again. You can also create your backup and recovery key in the \"Security and Privacy\" section of the menu that pops up by clicking on your avatar.</p>"},{"location":"org/chat/#spaces-and-channels","title":"Spaces and Channels","text":"<p>Spaces in Matrix are basically (nested) groups of subspaces, channels, and users. We have a space for the Bioinformatics Unit that groups our users and channels. Please contact <code>@tdido:matrix.org</code> to gain access to the Unit's space.</p>"}]}