{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CNIO Bioinformatics Unit documentation","text":"<p>Welcome to the documentation repository of the CNIO Bioinformatics Unit.</p> <ul> <li> <p>To navigate the documentation just use the menu on the left side of the page.</p> </li> <li> <p>You can check the source code that generates this site here.</p> </li> </ul>"},{"location":"dev/","title":"Guidelines for collaboration when writing code","text":"<p>Here are some guidelines for writing and managing code. You should be familiar with all the concepts in all the text and links in this page (except for the examples for code structure, which are on a need-to-know basis, and the source pages from the extracted paragraphs that you will find pasted inline).</p>"},{"location":"dev/#using-version-control-git","title":"Using version control (git)","text":"<p>We use git for version control. If you're not familiar with git, this is a good place to start, and this is a fantastic follow-up. </p> <p>There are some good interactive resources to learn git in general, and git branching in particular.</p>"},{"location":"dev/#where-to-put-your-code","title":"Where to put your code","text":"<p>We have a github group for the Unit for hosting and managing our code.</p> <p>If you're a member or collaborator of the Bioinformatics Unit and wish to gain access, you simply need to create a GitHub user and either contact a BU member, or create an issue to request access.</p>"},{"location":"dev/#how-to-organise-and-manage-your-code","title":"How to organise and manage your code","text":"<p>In order to facilitate collaboration when more than one person are working on a project, we follow some rules for repository structure and git-related operations (branches, commits, merge requests, etc.).</p>"},{"location":"dev/#project-structure","title":"Project structure","text":"<p>The structure of the repository will be defined by the project owner, ideally following well-established guidelines (see examples for Snakemake, Python, and R.</p>"},{"location":"dev/#git-workflow","title":"git workflow","text":"<p>For git, we follow the GitHub flow model. It's a simple and concise model that (at the time of writing) perfectly suits our requirements.</p>"},{"location":"dev/#commits","title":"Commits","text":"<p>Some more details on how we do commits and merge them into master are available here.</p>"},{"location":"dev/#merge-requests","title":"Merge requests","text":"<p>Start here to understand why merge requests, similarly to commits, need to be \"atomic\".</p> <p>Here are two paragraphs extracted from other sites with some additional details on merge requests:</p> <p>Adapted from https://alexsav.io/git-collaboration-guidelines.html</p> <p>The Pull or Merge Requests (PR) are used to share with your co-developers the code changes you did in your branch and ask them to review it. Here we always do a PR from your branch to the \u2018master\u2019 branch. In a PR, your colleagues will have a view of the differences between the branches and the commits. You must also add a title and a description. The title should be sufficient to understand what is being changed. In the description you should: - make a useful description, - describe what was changed in the pull request, - explain why this PR exists, - make it clear how it does what it sets out to do. E.g: Does it change a column in the database? How is this being done? What happens to the old data? - you may want to use screenshots to demonstrate what has changed if there is a GUI involved in the project.</p> <p>Single Responsibility Principle: The pull request should do only 1 thing.</p> <p>Pull request size: It should be small. The pull request must have a maximum of approximately 250 lines of change.</p> <p>Feature breaking: Whenever it\u2019s possible break pull requests into smaller ones.</p> <p>Adapted from https://yalantis.com/blog/code-review-via-gitlab-merge-requests-code-review-must/</p> <p>To minimize the time spent on reviewing each merge request, you need to have a strategy for code review.</p> <p>In our humble opinion, a good developer is not just someone who follows a programming workflow and writes high-quality code. A good developer knows how to deliver code for review and make the whole code review process effortless for the reviewer.</p> <p>Keep in mind that a good merge request should solve a specific task. We suggest not including more than one feature in one merge request. It would be much better if you created several merge requests instead.</p> <p>As a reviewer, don\u2019t hesitate to pull the source branch and test incoming code by yourself, especially if the merge request contains plenty of changes. Build the project and check that everything works as expected.</p> <p>Also, an important detail in our code review checklist is deleting branches when they\u2019re no longer needed. The responsibility for deleting branches after they\u2019re fully merged lies with the reviewer.</p>"},{"location":"hpc/admin/","title":"Cluster administration","text":""},{"location":"hpc/admin/#rack-layout","title":"Rack layout","text":"<p>Diagram can be edited with diagrams.net using the source code available here</p>"},{"location":"hpc/admin/#checking-and-restarting-nodes","title":"Checking and restarting nodes","text":"<p>For a full list of all available nodes, type:</p> <pre><code>scontrol show node\n</code></pre> <p>A given node status can be checked out with:</p> <pre><code>scontrol show node NODENAME\n</code></pre> <p>Sometimes, a node state will change to DRAIN for multiple reasons: i.e. slurm could not gracefully kill a job on a given node. Drained nodes won't handle incoming jobs until they are manually reset. To do so, the following command must be typed with admin privileges: </p> <pre><code>scontrol update NodeName=NODENAME State=RESUME\n</code></pre>"},{"location":"hpc/usage/","title":"The CNIO HPC cluster","text":"<p>These are some guidelines to using our HPC cluster and the Slurm workflow system.</p>"},{"location":"hpc/usage/#cluster-resources","title":"Cluster resources","text":""},{"location":"hpc/usage/#compute-nodes","title":"Compute nodes","text":"<p>The cluster currently features 11 compute nodes with the following configurations:</p> Count Node names CPU cores RAM GPUs 6 bc00[2-7] 52 512Gb - 3 bc00[8-10] 128 1Tb - 1 hm001 224 2Tb - 1 gp001 112 768Gb 3 x Nvidia A100 80Gb <p></p>"},{"location":"hpc/usage/#storage","title":"Storage","text":"<p>The cluster features 54Tb of standard storage (where user's home directories are mounted) and 512Tb of high performance storage (for compute jobs input and output).</p>"},{"location":"hpc/usage/#paragraph-for-adding-to-grant-applications-etc","title":"Paragraph for adding to grant applications, etc.","text":"<p>The CNIO HPC cluster currently consists of: two login nodes working in active/passive mode (2x 40 core/392Gb RAM); a total of 728 compute cores distributed in 9 \"standard\" compute nodes (6x 52 core/512Gb RAM and 3x 64core/768Gb RAM) and a high-memory node (1x 224 core/2Tb RAM); a dedicated GPU node featuring 3 x Nvidia A100 80Gb GPUs. Local storage is composed of 48 4TB disks in a dual-channel enclosure organized into a RAID-10 unit with an effective available storage of 54TB which host the home directories, and a high-performance lustre storage system with 512 TB of effective available storage for computation. The cluster runs a Slurm queuing system with fairshare priority management.</p>"},{"location":"hpc/usage/#cluster-usage","title":"Cluster usage","text":""},{"location":"hpc/usage/#introduction-to-hpc","title":"Introduction to HPC","text":"<p>See this tutorial for an introduction to HPC concepts and usage (highly recommended if you have little or no experience in using HPC environments).</p>"},{"location":"hpc/usage/#requesting-access","title":"Requesting access","text":"<p>Please fill out the access request form in order to  request access to the cluster.</p>"},{"location":"hpc/usage/#accessing-the-cluster","title":"Accessing the cluster","text":"<p>The cluster's hostname is <code>cluster1.cnio.es</code> (<code>cluster1</code> for short), and can be accessed via SSH.</p> <p>Note</p> <p>If you'd like to SSH into the cluster through the CNIO VPN you will need to ask IT for VPN access.</p>"},{"location":"hpc/usage/#mailing-list","title":"Mailing list","text":"<p>Important notifications about the cluster are sent to the CNIO HPC mailing list. Make sure to subscribe using this link to stay up to date. The list has very low traffic.</p> <p>Note</p> <p>Do subscribe to the list!</p>"},{"location":"hpc/usage/#storage_1","title":"Storage","text":"<p>Warning</p> <p>There are two types of storage in the cluster: your home directory and scratch space.</p> <p>Your home directory is well suited for storing software (e.g. installing conda), configuration files, etc. It is however not quick enough to store the input or output files of your analyses. Using your home for this will likely hang your jobs and corrupt your files. You should use the scratch space instead.</p> <p>Warning</p> <p>Scratch space is to be considered \"volatile\": you should copy your input files, execute, copy your output files, and delete everything.</p> <p>Home directories are not volatile, but data safety is not guaranteed in the long term (so keep backups).</p>"},{"location":"hpc/usage/#home-directories","title":"Home directories","text":"<p>You have an allocation of 300Gb in your home directory, located in <code>/home/&lt;yourusername&gt;/</code>.</p>"},{"location":"hpc/usage/#scratch-space","title":"Scratch space","text":""},{"location":"hpc/usage/#user-space","title":"User space","text":"<p>You have an allocation of 500Gb in your \"scratch\" directory, located at <code>/storage/scratch01/users/&lt;yourusername&gt;/</code>.</p> <p>This space is meant to be used for testing and ephemeral compute operations.</p>"},{"location":"hpc/usage/#project-space","title":"Project space","text":"<p>To request high-performance scratch space for a new project please fill the storage request form.</p>"},{"location":"hpc/usage/#checking-your-quotas","title":"Checking your quotas","text":"<p>You can check your current quotas with the following commands:</p> <pre><code>$ zfs get -r -o name,property,value userquota@$(whoami),userused@$(whoami) homepool2/home\n\n$ quotr quota list --long # check your scratch quotas\n\n</code></pre>"},{"location":"hpc/usage/#copying-data-tofrom-the-cluster","title":"Copying data to/from the cluster","text":"<p>The most efficient way of copying large amounts of data is by using <code>rsync</code>. <code>rsync</code> allows you to resume a transfer in case something goes wrong.</p> <p>To transfer a directory to the cluster using <code>rsync</code> you would do something like:</p> <pre><code>rsync -avx --progress mydirectory/ cluster1:/storage/scratch01/myuser/mydirectory/\n</code></pre> <p>Note</p> <p>Pay attention to the trailing slashes, which completely change rsync's behaviour if missing.</p> <p>For small transfers you could also use <code>scp</code> if you prefer:</p> <pre><code>scp -r mydirectory/ cluster1:/storage/scratch01/myuser/\n</code></pre> <p>In both cases you can revert the order of the local and remote directory to copy from the cluster to your local computer instead.</p>"},{"location":"hpc/usage/#submitting-jobs","title":"Submitting jobs","text":"<p>Warning</p> <p>As a general rule, no heavy processes should be run directly on the login nodes. Instead you should use the \"sbatch\" or \"srun\" commands to send them to the compute nodes. See examples below.</p> <p>The command structure to send a job to the queue is the following:</p> <pre><code>sbatch -o &lt;logfile&gt; -e &lt;errfile&gt; -J &lt;jobname&gt; -c &lt;ncores&gt; --mem=&lt;total_memory&gt;G \\\n    -t&lt;time_minutes&gt; --wrap \"&lt;command&gt;\"\n</code></pre> <p>For example, to submit the command <code>bwa index mygenome.fasta</code> as a job: </p> <pre><code>sbatch -o log.txt -e error.txt -J index_genome -c 1 --mem=4G -t120 \\\n    --wrap \"bwa index mygenome.fasta\"\n</code></pre>"},{"location":"hpc/usage/#job-resources","title":"Job resources","text":"<p><code>-c</code>, <code>--mem</code>, and <code>-t</code> tell the system how many cores, RAM memory, and time your job will need.</p> <p>Note</p> <p>Unless otherwise specified, jobs will get a default of 1 core, 2Gb of RAM, and a time limit of 30 minutes.</p>"},{"location":"hpc/usage/#thread-management","title":"Thread management","text":"<p>Many programs can spawn additional threads beyond what you explicitly request through the <code>-c</code> parameter. This can lead to oversubscription of CPU resources and negatively impact both your job and others running on the same node. For example, some Python libraries (like NumPy), R packages, or bioinformatics tools might automatically use multiple threads based on the available CPU cores, regardless of your Slurm allocation.</p> <p>To prevent this:</p> <ul> <li>Look for program-specific thread control options (e.g., <code>--threads</code>, <code>-t</code>, etc.)</li> <li>If applicable, consider setting environment variables like <code>OMP_NUM_THREADS</code>, <code>OPENBLAS_NUM_THREADS</code>, <code>MKL_NUM_THREADS</code>, and <code>NUMEXPR_NUM_THREADS</code> to match your requested cores</li> <li>When using workflow managers like Snakemake or Nextflow, ensure they properly propagate resource constraints to their child processes</li> </ul> <p>Please check this message on the mailing list for more details and an example.</p>"},{"location":"hpc/usage/#time-limits","title":"Time limits","text":"<p>Note</p> <p>Ideally, you should aim at your jobs having a duration of between 1 and 8 hours. Shorter jobs may cause too much scheduling overhead, and longer jobs will make it harder for the scheduler to optimally schedule jobs into the queues.</p> <p>The \"short\" queue has a limit of 2 hours per job and the highest priority (i.e. jobs in this queue will run sooner compared to other queues).</p> <p>The \"main\" queue has a limit of 24 hours per job and medium priority. </p> <p>The \"long\" queue has a limit of 168 hours (7 days) per job and 4 concurrent jobs, and the lowest priority.</p> <p>Note</p> <p>You can specify the partition (queue) you want to use with the <code>-p</code> argument to <code>sbatch</code> or <code>srun</code>.</p>"},{"location":"hpc/usage/#resources-and-their-effect-on-job-priority","title":"Resources and their effect on job priority","text":"<p>The resources you request for a job will influence the chances that such job has to enter the queue, compared to others: the more resources you request, the longer you may have to wait for those resources to be available.</p> <p>In addition, the future priority of your jobs will also be influenced by the resources you request (not use, request, even if you don't use them in the end): the more you request, the less priority you'll have for future jobs.</p> <p>If one of your jobs has lower priority than another one, but its running time would not delay that higher priority one from entering the queue and there are enough resources for it, your job could enter the queue first. That's why it's important to try and assign reasonably accurate time limits (a.k.a. walltimes) to your jobs.</p> <p>You can check how efficiently a job used its assigned resources with the <code>seff &lt;jobid&gt;</code> command (once the job is finished).</p>"},{"location":"hpc/usage/#snakemake-executor-for-slurm","title":"Snakemake executor for Slurm","text":"<p>Warning</p> <p>The previous way of having Snakemake send jobs to the nodes using a profile (<code>--profile $SMK_PROFILE_SLURM</code>) is still functional but deprecated. You should ideally move to the native executor described below, and report any issues to the list. </p> <p>Snakemake features a Slurm executor plugin, which will send jobs to Slurm instead of running them locally. After installing it, simply add the <code>--executor slurm</code> argument to your <code>snakemake</code> command.</p> <p>Note</p> <p>In order to indicate the resources required by each Snakemake rule you should use the threads and resources options.</p> <p>Note</p> <p>The Snakemake command will remain active while your jobs run, so it's recommended that you launch it inside a detachable terminal emulator (e.g. GNU Screen) so you can disconnect from the cluster and keep your jobs running.</p>"},{"location":"hpc/usage/#getting-information-for-jobs-submitted-via-snakemake","title":"Getting information for jobs submitted via Snakemake","text":"<p>Text extracted from the Snakemake Slurm executor docs</p> <p>The executor plugin for SLURM uses unique job names to inquire about job status. It ensures inquiring about job status for the series of jobs of a workflow does not put too much strain on the batch system\u2019s database. Human readable information is stored in the comment of a particular job. It is a combination of the rule name and wildcards. You can ask for it with the sacct or squeue commands, for example:</p> <pre><code>sacct -o JobID,State,Comment%40\n</code></pre> <p>Note, the \u201c%40\u201d after Comment ensures a width of 40 characters. This setting may be changed at will. If the width is too small, SLURM will abbreviate the column with a + sign.</p> <p>For running jobs, you can use the squeue command:</p> <pre><code>squeue -u $USER -o %i,%P,%.10j,%.40k\n</code></pre> <p>Here, the . settings for the ID and the comment ensure a sufficient width, too."},{"location":"hpc/usage/#interactive-sessions","title":"Interactive sessions","text":"<p>During development and testing you may want to be able to run commands interactively. You can request an interactive session, which will run on a compute node, using the <code>srun</code> command with the <code>--pty</code> argument.</p> <p>For example, to request a 2-hour session with 4Gb RAM and 2 CPUs, you would do:</p> <p><code>srun --mem=4096 -c 2 -t 120 --pty /bin/bash</code></p> <p>Note</p> <p>Interactive sessions are limited to a maximum of 120 minutes.</p>"},{"location":"hpc/usage/#gpus","title":"GPUs","text":"<p>Warning</p> <p>GPU usage is \"experimental\". Please expect changes in the setup (which will be announced through the mailing list).</p> <p>The cluster features three Nvidia A100 GPUs with 80Gb of VRAM each.</p> <p>To run a command using GPUs you need to specify the <code>gpu</code> partition, and request the number of GPUs you require using the <code>--gres=gpu:A100:&lt;number_of_gpus&gt;</code> argument of <code>sbatch</code>. Here's an example to run a python script with one GPU:</p> <p><code>sbatch -p gpu --gres=gpu:A100:1 --wrap \"python train_my_net.py\"</code></p>"},{"location":"hpc/usage/#installing-software-with-gpu-support","title":"Installing software with GPU support","text":"<p>When installing software that requires GPU support (e.g., deep learning frameworks with CUDA), it's important to perform the installation on a GPU node rather than on the head node. Installing on the head node, which lacks GPUs, may result in CPU-only versions of libraries being installed automatically, even when GPU-enabled versions are available.</p> <p>There are two recommended approaches for installing GPU-enabled software:</p> <p>Option 1: Interactive session on GPU node</p> <p>Request an interactive session on the GPU node to install software or create conda environments:</p> <pre><code>srun -p gpu --gres=gpu:A100:1 --mem=8192 -c 4 -t 120 --pty /bin/bash\n</code></pre> <p>Once connected to the GPU node, verify GPU access:</p> <pre><code>nvidia-smi\n</code></pre> <p>Then proceed with your software installation (e.g., creating conda environments, installing packages with pip/conda).</p> <p>Option 2: Installation via batch job</p> <p>Alternatively, you can submit the installation commands as a batch job that runs on the GPU node:</p> <pre><code>sbatch -p gpu --gres=gpu:A100:1 --mem=8192 -c 4 -t 60 --wrap \"conda create -n myenv python=3.10 &amp;&amp; conda activate myenv &amp;&amp; conda install &lt;packages&gt;\"\n</code></pre> <p>Verifying GPU access</p> <p>After installation, verify that your software can access the GPU by running <code>nvidia-smi</code> or using framework-specific commands to check CUDA availability. This ensures that the GPU-enabled versions were correctly installed.</p>"},{"location":"hpc/usage/#installing-software","title":"Installing software","text":"<p>Software management is left up to the user, and we recommend doing it by installing miniforge and the bioconda channels.</p> <p>If you're completely unfamiliar with conda and bioconda, we recommend following this tutorial.</p> <p>Note</p> <p>Remember to use your home directory to install software (including conda), as lustre performs poorly when reading small files often.</p>"},{"location":"org/calendar/","title":"Outlook Calendar","text":"<p>CNIO provides access to Microsoft 365 and its suite of productivity tools. The Bioinformatics Unit calendar centralises all events that are relevant to the whole Unit.</p> <p>Warning</p> <p>The Bioinformatics Unit group calendar may not be visible for you by default. Make sure you enable it in Outlook by selecting it in your list of calendars.</p>"},{"location":"org/calendar/#creating-events","title":"Creating Events","text":"<p>The idea is for the person responsible for an event to add it to the calendar, inviting those directly involved. The rest of the Unit members will then be aware of what's going on by having the events visible on their calendars.</p> <p>In short:</p> <ul> <li>Events of general interest to the Unit (lab meetings, project meetings, conferences, etc.) should be created in the Bioinformatics Unit group calendar.</li> <li>The person responsible for a meeting should be the one creating the calendar event.</li> <li>People directly involved should be invited to the event.</li> <li>The \"Teams\" meeting option should be enabled to allow for remote participation (unless otherwise agreed).</li> </ul>"},{"location":"org/calendar/#synchronisation-with-3rd-party-clients","title":"Synchronisation with 3rd Party Clients","text":"<p>Group calendars are not exposed by the Exchange protocol, and therefore can't be synced with 3rd party clients.</p> <p>A workaround for this is to click on the event and select \"Add to my calendar\". This should copy the event to your personal calendar and keep it in sync (YMMV, as this is a Microsoft).</p>"},{"location":"org/chat/","title":"Matrix Chat","text":"<p>We use Matrix Chat for internal communications within the Bioinformatics Unit.</p> <p>Matrix is a federated chat protocol that allows secure, decentralized communication. Users can create accounts on one server and join channels from any server on the federation.</p>"},{"location":"org/chat/#creating-an-account","title":"Creating an Account","text":"<p>To create an account, choose one of the available Matrix clients. The web-based version of Element is a good place to start. Choose the public matrix.org server when creating your account. After creating your account, feel free to install the Android or iOS versions, or explore other clients.</p> <p>Warning</p> <p>Make sure to create a backup of your encryption keys and keep the associated recovery key safe. Otherwise, you won't be able to access old messages after logging in again. You can also create your backup and recovery key in the \"Security and Privacy\" section of the menu that pops up by clicking on your avatar.</p>"},{"location":"org/chat/#spaces-and-channels","title":"Spaces and Channels","text":"<p>Spaces in Matrix are basically (nested) groups of subspaces, channels, and users. We have a space for the Bioinformatics Unit that groups our users and channels. Please contact <code>@tdido:matrix.org</code> to gain access to the Unit's space.</p>"}]}